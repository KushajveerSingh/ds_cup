{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6f8ee94c255eb1f45edb80e83721093c1db1e2ea85447c0854292673b957abb8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/KushajveerSingh/ds_cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score,precision_score\n",
    "smallestF =  np.finfo('float').eps # smallest float value (numpy), will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../data_orig')\n",
    "# path = Path('ds_cup/orig_data')\n",
    "orig_train_df = pd.read_csv(path/'train.csv')\n",
    "orig_valid_df = pd.read_csv(path/'valid.csv')\n",
    "orig_test_df = pd.read_csv(path/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix Name error:\n",
    "orig_train_df.rename(columns={'auto_open_ 36_month_num': 'auto_open_36_month_num'},inplace=True)\n",
    "orig_valid_df.rename(columns={'auto_open_ 36_month_num': 'auto_open_36_month_num'},inplace=True)\n",
    "orig_test_df.rename(columns={'auto_open_ 36_month_num': 'auto_open_36_month_num'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size(df, name):\n",
    "    counts = df['Default_ind'].value_counts()\n",
    "    print(f'{name} dataset')\n",
    "    print(f'Num 0 values = {counts[0]}')\n",
    "    print(f'Num 1 values = {counts[1]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_size(orig_train_df, 'train')\n",
    "print_size(orig_valid_df, 'valid')\n",
    "print_size(orig_test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orig_train_df.isnull().sum())\n",
    "print('--')\n",
    "print(orig_valid_df.isnull().sum())\n",
    "print('--')\n",
    "print(orig_test_df.isnull().sum())\n",
    "print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createThreshDict(colName,Arr):\n",
    "    _dict = {}\n",
    "    _dict['colName'] = colName\n",
    "    _dict['ThreshArr'] = Arr\n",
    "    return _dict\n",
    "def binColumn(col,threshArr):\n",
    "    assert col.isnull().all() != True, \"All values are null?\"\n",
    "    def binVal(val):\n",
    "        loc = 1\n",
    "        for x in threshArr:\n",
    "            if(val<=x):\n",
    "                return loc\n",
    "            else:\n",
    "                loc = loc+1\n",
    "        return loc\n",
    "    theCol = col.copy()\n",
    "    threshArr = np.sort(np.array(threshArr))\n",
    "    #Replace Nulls with 0 if any\n",
    "    theBinnedCol = []\n",
    "    for val in theCol:\n",
    "        if(not np.isnan(val)):\n",
    "            theBinnedCol.extend([binVal(val)])\n",
    "        else:\n",
    "            theBinnedCol.extend([0])\n",
    "        #theCol.update(pd.Series([], index=[ind]))\n",
    "    return pd.Series(theBinnedCol,index=col.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDataBinClean(DF):\n",
    "    DFThresh =  pd.DataFrame(columns=createThreshDict(\"\",[]).keys())\n",
    "    DF_Cleaned = pd.DataFrame(columns=DF.columns)\n",
    "    #--'tot_credit_debt'--\n",
    "    _col = 'tot_credit_debt'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'avg_card_debt'--\n",
    "    cutoff_avg_card_debt = (13167.825020 + 99999.0)/2\n",
    "    _col = 'avg_card_debt'\n",
    "    _thresh = DF[DF.avg_card_debt < cutoff_avg_card_debt]['avg_card_debt'].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    _thresh = np.append(_thresh, [cutoff_avg_card_debt])\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'credit_age'--\n",
    "    _col = 'credit_age'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'credit_good_age'--\n",
    "    _col = 'credit_good_age'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'card_age'--\n",
    "    _col = 'card_age'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'non_mtg_acc_past_due_12_months_num'--\n",
    "    DF_Cleaned['non_mtg_acc_past_due_12_months_num'] = DF['non_mtg_acc_past_due_12_months_num'].copy()\n",
    "    #--'non_mtg_acc_past_due_6_months_num'--\n",
    "    DF_Cleaned['non_mtg_acc_past_due_6_months_num'] = DF['non_mtg_acc_past_due_6_months_num'].copy()\n",
    "    #--'mortgages_past_due_6_months_num'--\n",
    "    DF_Cleaned['mortgages_past_due_6_months_num'] = DF['mortgages_past_due_6_months_num'].copy()\n",
    "    #--'credit_past_due_amount'--\n",
    "    _col = 'credit_past_due_amount'\n",
    "    _thresh = DF[DF.credit_past_due_amount > 0][_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    _thresh = np.insert(_thresh, 0, 0+smallestF)\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'inq_12_month_num'--\n",
    "    _col = 'inq_12_month_num'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'card_inq_24_month_num'--\n",
    "    _col = 'card_inq_24_month_num'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'card_open_36_month_num'--\n",
    "    _col = 'card_open_36_month_num'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--'auto_open_ 36_month_num'--\n",
    "    _col = 'auto_open_36_month_num'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--'uti_card'--\n",
    "    _col = 'uti_card'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'uti_50plus_pct'--\n",
    "    _col = 'uti_50plus_pct'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'uti_max_credit_line'--\n",
    "    _col = 'uti_max_credit_line'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'uti_card_50plus_pct'--\n",
    "    _col = 'uti_card_50plus_pct'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'ind_acc_XYZ'--\n",
    "    _col = 'ind_acc_XYZ'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--'rep_income'--\n",
    "    _col = 'rep_income'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'States'--\n",
    "    _col = 'States'\n",
    "    enc = OneHotEncoder(handle_unknown='ignore',sparse = False)\n",
    "    SM = enc.fit_transform(DF[_col].to_numpy().reshape(-1,1)) #Got it!\n",
    "    for ind,cat in zip(range(0,len(enc.categories_[0])),enc.categories_[0]):\n",
    "        DF_Cleaned.drop(columns=['is'+str(cat)],inplace=True,errors='ignore')\n",
    "        DF_Cleaned.insert(0,'is'+str(cat), SM[:,ind]) # Inserted column names are isAK, isAL, isDC..etc. \n",
    "    #And it will contain 0 if that is not the state and 1 if that is the state,\n",
    "    DF_Cleaned.drop(columns=[_col],inplace=True) #Remove States Column\n",
    "    #--'Default_ind'--\n",
    "    _col = 'Default_ind'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--\n",
    "    return DF_Cleaned,DFThresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binDataFrameWithThresh(DF,ThreshDF):\n",
    "    DF_Cleaned = DF.copy()\n",
    "    for col,arr in zip(ThreshDF.colName.values,ThreshDF.ThreshArr.values):\n",
    "        DF_Cleaned[col] = binColumn(DF[col],arr)\n",
    "    return DF_Cleaned\n",
    "def oneHotEncodeStates(DF):\n",
    "    _col = 'States'\n",
    "    DF_Fixed = DF.copy()\n",
    "    enc = OneHotEncoder(handle_unknown='ignore',sparse = False)\n",
    "    SM = enc.fit_transform(DF[_col].to_numpy().reshape(-1,1)) #Got it!\n",
    "    for ind,cat in zip(range(0,len(enc.categories_[0])),enc.categories_[0]):\n",
    "        DF_Fixed.drop(columns=['is'+str(cat)],inplace=True,errors='ignore')\n",
    "        DF_Fixed.insert(0,'is'+str(cat), SM[:,ind]) # Inserted column names are isAK, isAL, isDC..etc. \n",
    "    #And it will contain 0 if that is not the state and 1 if that is the state,\n",
    "    DF_Fixed.drop(columns=[_col],inplace=True) #Remove States Column\n",
    "    return DF_Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_thresh = trainDataBinClean(orig_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = binDataFrameWithThresh(orig_valid_df,df_thresh)\n",
    "df_valid = oneHotEncodeStates(df_valid)\n",
    "df_test = binDataFrameWithThresh(orig_test_df,df_thresh)\n",
    "df_test = oneHotEncodeStates(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1) #Shuffle!\n",
    "df_trainX = df_train.iloc[:,:-1]\n",
    "df_trainY = df_train.iloc[:,-1]\n",
    "#\n",
    "df_valid = df_train.sample(frac=1)\n",
    "df_validX = df_valid.iloc[:,:-1]\n",
    "df_validY = df_valid.iloc[:,-1]\n",
    "#\n",
    "df_test = df_train.sample(frac=1)\n",
    "df_testX = df_test.iloc[:,:-1]\n",
    "df_testY = df_test.iloc[:,-1]"
   ]
  },
  {
   "source": [
    "# --- Dataframes Ready ---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight='balanced',solver='liblinear')\n",
    "scores = cross_validate(model, df_trainX, df_trainY, cv=10,scoring=('accuracy','recall','precision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_scores = pd.DataFrame(scores)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average Cross Validation Results')\n",
    "print('Accuracy =',np.average(cv_scores.test_accuracy))\n",
    "print('Recall =',np.average(cv_scores.test_recall))\n",
    "print('Precision =',np.average(cv_scores.test_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(class_weight='balanced',solver='liblinear')\n",
    "hist = model2.fit(df_trainX,df_trainY)\n",
    "df_validYPred = model2.predict(df_validX)"
   ]
  },
  {
   "source": [
    "accScore = accuracy_score(df_validY,df_validYPred)\n",
    "recScore = recall_score(df_validY,df_validYPred)\n",
    "preScore = precision_score(df_validY,df_validYPred)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Accuracy =',accScore)\n",
    "print('Recall =',recScore)\n",
    "print('Precision =',preScore)"
   ]
  }
 ]
}