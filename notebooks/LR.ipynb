{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "6f8ee94c255eb1f45edb80e83721093c1db1e2ea85447c0854292673b957abb8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Cloning into 'ds_cup'...\nerror: cannot spawn sh: No such file or directory\nfatal: could not read Username for 'https://github.com': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/KushajveerSingh/ds_cup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import accuracy_score, recall_score,precision_score\n",
    "smallestF =  np.finfo('float').eps # smallest float value (numpy), will be used later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path('../data_orig')\n",
    "# path = Path('ds_cup/orig_data')\n",
    "orig_train_df = pd.read_csv(path/'train.csv')\n",
    "orig_valid_df = pd.read_csv(path/'valid.csv')\n",
    "orig_test_df = pd.read_csv(path/'test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fix Name error:\n",
    "orig_train_df.rename(columns={'auto_open_ 36_month_num': 'auto_open_36_month_num'},inplace=True)\n",
    "orig_valid_df.rename(columns={'auto_open_ 36_month_num': 'auto_open_36_month_num'},inplace=True)\n",
    "orig_test_df.rename(columns={'auto_open_ 36_month_num': 'auto_open_36_month_num'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size(df, name):\n",
    "    counts = df['Default_ind'].value_counts()\n",
    "    print(f'{name} dataset')\n",
    "    print(f'Num 0 values = {counts[0]}')\n",
    "    print(f'Num 1 values = {counts[1]}')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "train dataset\nNum 0 values = 18414\nNum 1 values = 1586\n\nvalid dataset\nNum 0 values = 2778\nNum 1 values = 222\n\ntest dataset\nNum 0 values = 4599\nNum 1 values = 401\n\n"
     ]
    }
   ],
   "source": [
    "print_size(orig_train_df, 'train')\n",
    "print_size(orig_valid_df, 'valid')\n",
    "print_size(orig_test_df, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "tot_credit_debt                          0\navg_card_debt                            0\ncredit_age                               0\ncredit_good_age                          0\ncard_age                                 0\nnon_mtg_acc_past_due_12_months_num       0\nnon_mtg_acc_past_due_6_months_num        0\nmortgages_past_due_6_months_num          0\ncredit_past_due_amount                   0\ninq_12_month_num                         0\ncard_inq_24_month_num                    0\ncard_open_36_month_num                   0\nauto_open_36_month_num                   0\nuti_card                                 0\nuti_50plus_pct                           0\nuti_max_credit_line                      0\nuti_card_50plus_pct                   2055\nind_acc_XYZ                              0\nrep_income                            1570\nStates                                   0\nDefault_ind                              0\ndtype: int64\n--\ntot_credit_debt                         0\navg_card_debt                           0\ncredit_age                              0\ncredit_good_age                         0\ncard_age                                0\nnon_mtg_acc_past_due_12_months_num      0\nnon_mtg_acc_past_due_6_months_num       0\nmortgages_past_due_6_months_num         0\ncredit_past_due_amount                  0\ninq_12_month_num                        0\ncard_inq_24_month_num                   0\ncard_open_36_month_num                  0\nauto_open_36_month_num                  0\nuti_card                                0\nuti_50plus_pct                          0\nuti_max_credit_line                     0\nuti_card_50plus_pct                   297\nind_acc_XYZ                             0\nrep_income                            253\nStates                                  0\nDefault_ind                             0\ndtype: int64\n--\ntot_credit_debt                         0\navg_card_debt                           0\ncredit_age                              0\ncredit_good_age                         0\ncard_age                                0\nnon_mtg_acc_past_due_12_months_num      0\nnon_mtg_acc_past_due_6_months_num       0\nmortgages_past_due_6_months_num         0\ncredit_past_due_amount                  0\ninq_12_month_num                        0\ncard_inq_24_month_num                   0\ncard_open_36_month_num                  0\nauto_open_36_month_num                  0\nuti_card                                0\nuti_50plus_pct                          0\nuti_max_credit_line                     0\nuti_card_50plus_pct                   499\nind_acc_XYZ                             0\nrep_income                            383\nStates                                  0\nDefault_ind                             0\ndtype: int64\n--\n"
     ]
    }
   ],
   "source": [
    "print(orig_train_df.isnull().sum())\n",
    "print('--')\n",
    "print(orig_valid_df.isnull().sum())\n",
    "print('--')\n",
    "print(orig_test_df.isnull().sum())\n",
    "print('--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createThreshDict(colName,Arr):\n",
    "    _dict = {}\n",
    "    _dict['colName'] = colName\n",
    "    _dict['ThreshArr'] = Arr\n",
    "    return _dict\n",
    "def binColumn(col,threshArr):\n",
    "    assert col.isnull().all() != True, \"All values are null?\"\n",
    "    def binVal(val):\n",
    "        loc = 1\n",
    "        for x in threshArr:\n",
    "            if(val<=x):\n",
    "                return loc\n",
    "            else:\n",
    "                loc = loc+1\n",
    "        return loc\n",
    "    theCol = col.copy()\n",
    "    threshArr = np.sort(np.array(threshArr))\n",
    "    #Replace Nulls with 0 if any\n",
    "    theBinnedCol = []\n",
    "    for val in theCol:\n",
    "        if(not np.isnan(val)):\n",
    "            theBinnedCol.extend([binVal(val)])\n",
    "        else:\n",
    "            theBinnedCol.extend([0])\n",
    "        #theCol.update(pd.Series([], index=[ind]))\n",
    "    return pd.Series(theBinnedCol,index=col.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainDataBinClean(DF):\n",
    "    DFThresh =  pd.DataFrame(columns=createThreshDict(\"\",[]).keys())\n",
    "    DF_Cleaned = pd.DataFrame(columns=DF.columns)\n",
    "    #--'tot_credit_debt'--\n",
    "    _col = 'tot_credit_debt'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'avg_card_debt'--\n",
    "    cutoff_avg_card_debt = (13167.825020 + 99999.0)/2\n",
    "    _col = 'avg_card_debt'\n",
    "    _thresh = DF[DF.avg_card_debt < cutoff_avg_card_debt]['avg_card_debt'].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    _thresh = np.append(_thresh, [cutoff_avg_card_debt])\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'credit_age'--\n",
    "    _col = 'credit_age'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'credit_good_age'--\n",
    "    _col = 'credit_good_age'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'card_age'--\n",
    "    _col = 'card_age'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'non_mtg_acc_past_due_12_months_num'--\n",
    "    DF_Cleaned['non_mtg_acc_past_due_12_months_num'] = DF['non_mtg_acc_past_due_12_months_num'].copy()\n",
    "    #--'non_mtg_acc_past_due_6_months_num'--\n",
    "    DF_Cleaned['non_mtg_acc_past_due_6_months_num'] = DF['non_mtg_acc_past_due_6_months_num'].copy()\n",
    "    #--'mortgages_past_due_6_months_num'--\n",
    "    DF_Cleaned['mortgages_past_due_6_months_num'] = DF['mortgages_past_due_6_months_num'].copy()\n",
    "    #--'credit_past_due_amount'--\n",
    "    _col = 'credit_past_due_amount'\n",
    "    _thresh = DF[DF.credit_past_due_amount > 0][_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    _thresh = np.insert(_thresh, 0, 0+smallestF)\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'inq_12_month_num'--\n",
    "    _col = 'inq_12_month_num'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'card_inq_24_month_num'--\n",
    "    _col = 'card_inq_24_month_num'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'card_open_36_month_num'--\n",
    "    _col = 'card_open_36_month_num'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--'auto_open_ 36_month_num'--\n",
    "    _col = 'auto_open_36_month_num'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--'uti_card'--\n",
    "    _col = 'uti_card'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'uti_50plus_pct'--\n",
    "    _col = 'uti_50plus_pct'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'uti_max_credit_line'--\n",
    "    _col = 'uti_max_credit_line'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'uti_card_50plus_pct'--\n",
    "    _col = 'uti_card_50plus_pct'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'ind_acc_XYZ'--\n",
    "    _col = 'ind_acc_XYZ'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--'rep_income'--\n",
    "    _col = 'rep_income'\n",
    "    _thresh = DF[_col].quantile([0.2,0.4,0.6,0.8]).to_numpy()\n",
    "    DFThresh = DFThresh.append(createThreshDict(_col,\n",
    "        _thresh),\n",
    "        ignore_index=True) \n",
    "    DF_Cleaned[_col] = binColumn(DF[_col],_thresh)\n",
    "    #--'States'--\n",
    "    _col = 'States'\n",
    "    enc = OneHotEncoder(handle_unknown='ignore',sparse = False)\n",
    "    SM = enc.fit_transform(DF[_col].to_numpy().reshape(-1,1)) #Got it!\n",
    "    for ind,cat in zip(range(0,len(enc.categories_[0])),enc.categories_[0]):\n",
    "        DF_Cleaned.drop(columns=['is'+str(cat)],inplace=True,errors='ignore')\n",
    "        DF_Cleaned.insert(0,'is'+str(cat), SM[:,ind]) # Inserted column names are isAK, isAL, isDC..etc. \n",
    "    #And it will contain 0 if that is not the state and 1 if that is the state,\n",
    "    DF_Cleaned.drop(columns=[_col],inplace=True) #Remove States Column\n",
    "    #--'Default_ind'--\n",
    "    _col = 'Default_ind'\n",
    "    DF_Cleaned[_col] = DF[_col].copy()\n",
    "    #--\n",
    "    return DF_Cleaned,DFThresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binDataFrameWithThresh(DF,ThreshDF):\n",
    "    DF_Cleaned = DF.copy()\n",
    "    for col,arr in zip(ThreshDF.colName.values,ThreshDF.ThreshArr.values):\n",
    "        DF_Cleaned[col] = binColumn(DF[col],arr)\n",
    "    return DF_Cleaned\n",
    "def oneHotEncodeStates(DF):\n",
    "    _col = 'States'\n",
    "    DF_Fixed = DF.copy()\n",
    "    enc = OneHotEncoder(handle_unknown='ignore',sparse = False)\n",
    "    SM = enc.fit_transform(DF[_col].to_numpy().reshape(-1,1)) #Got it!\n",
    "    for ind,cat in zip(range(0,len(enc.categories_[0])),enc.categories_[0]):\n",
    "        DF_Fixed.drop(columns=['is'+str(cat)],inplace=True,errors='ignore')\n",
    "        DF_Fixed.insert(0,'is'+str(cat), SM[:,ind]) # Inserted column names are isAK, isAL, isDC..etc. \n",
    "    #And it will contain 0 if that is not the state and 1 if that is the state,\n",
    "    DF_Fixed.drop(columns=[_col],inplace=True) #Remove States Column\n",
    "    return DF_Fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_thresh = trainDataBinClean(orig_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = binDataFrameWithThresh(orig_valid_df,df_thresh) # Binning Validation data using thresholds of training data\n",
    "df_valid = oneHotEncodeStates(df_valid) #States\n",
    "#Same ops on test data\n",
    "df_test = binDataFrameWithThresh(orig_test_df,df_thresh)\n",
    "df_test = oneHotEncodeStates(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_df.to_csv(path/'train_binned.csv',index=None)\n",
    "df_thresh.to_csv(path/'Thresholds.csv',index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.sample(frac=1) #Shuffle!\n",
    "df_trainX = df_train.iloc[:,:-1]\n",
    "df_trainY = df_train.iloc[:,-1]\n",
    "#\n",
    "df_valid = df_valid.sample(frac=1)\n",
    "df_validX = df_valid.iloc[:,:-1]\n",
    "df_validY = df_valid.iloc[:,-1]\n",
    "#\n",
    "df_test = df_test.sample(frac=1)\n",
    "df_testX = df_test.iloc[:,:-1]\n",
    "df_testY = df_test.iloc[:,-1]"
   ]
  },
  {
   "source": [
    "# --- Dataframes Ready ---"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Sample Logistic Regression Test"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(class_weight='balanced',solver='liblinear')\n",
    "scores = cross_validate(model, df_trainX, df_trainY, cv=10,scoring=('accuracy','recall','precision'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   fit_time  score_time  test_accuracy  test_recall  test_precision\n",
       "0  0.150033    0.005000         0.7755     0.651899        0.207243\n",
       "1  0.128054    0.004995         0.7630     0.639241        0.194981\n",
       "2  0.179446    0.006001         0.7605     0.664557        0.197740\n",
       "3  0.183745    0.006001         0.7410     0.632911        0.178571\n",
       "4  0.168128    0.006011         0.7400     0.584906        0.170018\n",
       "5  0.162833    0.005001         0.7550     0.622642        0.187146\n",
       "6  0.129343    0.006012         0.7665     0.622642        0.195652\n",
       "7  0.131008    0.006006         0.7450     0.735849        0.200000\n",
       "8  0.134957    0.005001         0.7600     0.742138        0.211849\n",
       "9  0.134367    0.004999         0.7595     0.660377        0.197368"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>fit_time</th>\n      <th>score_time</th>\n      <th>test_accuracy</th>\n      <th>test_recall</th>\n      <th>test_precision</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.150033</td>\n      <td>0.005000</td>\n      <td>0.7755</td>\n      <td>0.651899</td>\n      <td>0.207243</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.128054</td>\n      <td>0.004995</td>\n      <td>0.7630</td>\n      <td>0.639241</td>\n      <td>0.194981</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.179446</td>\n      <td>0.006001</td>\n      <td>0.7605</td>\n      <td>0.664557</td>\n      <td>0.197740</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.183745</td>\n      <td>0.006001</td>\n      <td>0.7410</td>\n      <td>0.632911</td>\n      <td>0.178571</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.168128</td>\n      <td>0.006011</td>\n      <td>0.7400</td>\n      <td>0.584906</td>\n      <td>0.170018</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.162833</td>\n      <td>0.005001</td>\n      <td>0.7550</td>\n      <td>0.622642</td>\n      <td>0.187146</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.129343</td>\n      <td>0.006012</td>\n      <td>0.7665</td>\n      <td>0.622642</td>\n      <td>0.195652</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.131008</td>\n      <td>0.006006</td>\n      <td>0.7450</td>\n      <td>0.735849</td>\n      <td>0.200000</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.134957</td>\n      <td>0.005001</td>\n      <td>0.7600</td>\n      <td>0.742138</td>\n      <td>0.211849</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.134367</td>\n      <td>0.004999</td>\n      <td>0.7595</td>\n      <td>0.660377</td>\n      <td>0.197368</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "cv_scores = pd.DataFrame(scores)\n",
    "cv_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Average Cross Validation Results\nAccuracy = 0.7565999999999999\nRecall = 0.6557161054056205\nPrecision = 0.19405693235688787\n"
     ]
    }
   ],
   "source": [
    "print('Average Cross Validation Results')\n",
    "print('Accuracy =',np.average(cv_scores.test_accuracy))\n",
    "print('Recall =',np.average(cv_scores.test_recall))\n",
    "print('Precision =',np.average(cv_scores.test_precision))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = LogisticRegression(class_weight='balanced',solver='liblinear')\n",
    "hist = model2.fit(df_trainX,df_trainY)\n",
    "df_validYPred = model2.predict(df_validX)"
   ]
  },
  {
   "source": [
    "accScore = accuracy_score(df_validY,df_validYPred)\n",
    "recScore = recall_score(df_validY,df_validYPred)\n",
    "preScore = precision_score(df_validY,df_validYPred)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Accuracy = 0.79\nRecall = 0.6081081081081081\nPrecision = 0.19911504424778761\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy =',accScore)\n",
    "print('Recall =',recScore)\n",
    "print('Precision =',preScore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Confusion Matrix\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Predicted   0.0  1.0\n",
       "Actual              \n",
       "0.0        2235  543\n",
       "1.0          87  135"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th>Predicted</th>\n      <th>0.0</th>\n      <th>1.0</th>\n    </tr>\n    <tr>\n      <th>Actual</th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0.0</th>\n      <td>2235</td>\n      <td>543</td>\n    </tr>\n    <tr>\n      <th>1.0</th>\n      <td>87</td>\n      <td>135</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "print('Confusion Matrix')\n",
    "pd.crosstab(pd.Series(df_validY.values,name='Actual'),pd.Series(df_validYPred,name='Predicted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       0.0\n",
       "1       0.0\n",
       "2       0.0\n",
       "3       0.0\n",
       "4       0.0\n",
       "       ... \n",
       "2995    0.0\n",
       "2996    0.0\n",
       "2997    0.0\n",
       "2998    0.0\n",
       "2999    0.0\n",
       "Name: Actual, Length: 3000, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": []
  }
 ]
}